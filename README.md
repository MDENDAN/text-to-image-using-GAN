# Text-to-Image-Synthesis 

## Intoduction

This project is mainly inspired from [Generative Adversarial Text-to-Image Synthesis paper](https://arxiv.org/abs/1605.05396). We implemented this model using PyTorch. In this model we train a conditional generative adversarial network, conditioned on text captions, to generate images that correspond to the captions. The network architecture is shown below (Image from [1]). This architecture is based on DCGAN. 

<figure><img src='images/dcgan_network.png'></figure>
Image credits [1]

## Datasets

We used the hdf5 format of these datasets which can be found here for **birds_hdf5** (https://drive.google.com/file/d/1mNhn6MYpBb-JwE86GC1kk0VJsYj-Pn5j/view) and here for **flowers_hdf5**
(https://drive.google.com/file/d/1EgnaTrlHGaqK5CCgHKLclZMT_AMSTyh8/view). These hdf5 datasets were converted from **[Caltech-UCSD Birds 200]**(http://www.vision.caltech.edu/visipedia/CUB-200.html) and **[Oxford Flowers]**(http://www.robots.ox.ac.uk/~vgg/data/flowers/102/) datasets.

We used the [text embeddings](https://github.com/reedscot/icml2016) provided by the paper authors. 

**Hd5 file taxonomy**
`
 - split (train | valid | test )
    - example_name
      - 'name'
      - 'img'
      - 'embeddings'
      - 'class'
      - 'txt'

**To use this code for training you can:** <br/>
$ git clone https://github.com/Rakshith-Manandi/text-to-image-using-GAN.git <br/>
$ cd ./text-to-image-using-GAN <br/>
$ python runtime.py <br/>

**To get a glimpse of the model you can:** <br/>
$ git clone https://github.com/Rakshith-Manandi/text-to-image-using-GAN.git <br/>
$ cd ./text-to-image-using-GAN <br/>
$ jupyter notebook GAN_demo.ipynb <br/>

**Here are a few examples of the images generated by our model:** <br/>
<p align='center'>
<figure><img src = 'images/succcess_birds.PNG'></figure>
</p>
<p align='center'>
<figure><img src = 'images/succcess_flowers.PNG'></figure>
</p>


      
 
